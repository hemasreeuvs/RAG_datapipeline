{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95eaaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fe026f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Hema', 'date_created': '02-18-2026'}, page_content='this is the main text content Iam using to Create RAG')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document (\n",
    "    page_content = 'this is the main text content Iam using to Create RAG',\n",
    "    metadata ={\n",
    "        \"source\" : \"example.txt\",\n",
    "        \"pages\" : 1,\n",
    "        \"author\": \"Hema\",\n",
    "        \"date_created\" : \"02-18-2026\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e4b3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de7af6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text file created\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"../data/text_files/rag_intro.txt\": \n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation (RAG) is an architectural framework designed to enhance the accuracy and reliability of Large Language Models (LLMs) by grounding them in external, authoritative knowledge bases. While standard models rely strictly on the data they were originally trained on—which can lead to outdated information or \"hallucinations\"—RAG enables a model to look up fresh data before generating a response. This process involves retrieving relevant document snippets from a source like a private database or the live web, and then passing that context into the model alongside the user’s original question.\"\"\" ,\n",
    "   \n",
    "   \"../data/text_files/langchain_intro.txt\":  \"\"\" Langchain Intro\n",
    "\n",
    "LangChain serves as the primary \"operating system\" or toolkit for building these RAG systems. It provides a modular library of \"LEGO-like\" components—such as document loaders, text splitters, and vector store connectors—that simplify the complex plumbing required to link an LLM to external data. Instead of developers manually coding every step of the retrieval process, LangChain allows them to create \"chains\" that automate the flow of information from ingestion to the final generated answer. \n",
    "The synergy between the two lies in how LangChain orchestrates the RAG pipeline. Using LangChain, a developer can quickly load diverse data formats like PDFs or website content, break them into manageable chunks, and store them as mathematical \"embeddings\" in a vector database for rapid searching. When a user asks a question, LangChain's retrieval modules find the most relevant pieces of information and prompt the LLM to use that specific data to craft a factually grounded and contextually correct response. \n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath , 'w' , encoding = \"utf-8\") as f:\n",
    "        f.write(content)\n",
    "print(\"sample text file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07ffe8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/rag_intro.txt'}, page_content='\\n    Retrieval-Augmented Generation (RAG) is an architectural framework designed to enhance the accuracy and reliability of Large Language Models (LLMs) by grounding them in external, authoritative knowledge bases. While standard models rely strictly on the data they were originally trained on—which can lead to outdated information or \"hallucinations\"—RAG enables a model to look up fresh data before generating a response. This process involves retrieving relevant document snippets from a source like a private database or the live web, and then passing that context into the model alongside the user’s original question.')]\n"
     ]
    }
   ],
   "source": [
    "##Text Loader\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader (\"../data/text_files/rag_intro.txt\" , encoding = \"utf-8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e7a4216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\langchain_intro.txt'}, page_content=' Langchain Intro\\n\\nLangChain serves as the primary \"operating system\" or toolkit for building these RAG systems. It provides a modular library of \"LEGO-like\" components—such as document loaders, text splitters, and vector store connectors—that simplify the complex plumbing required to link an LLM to external data. Instead of developers manually coding every step of the retrieval process, LangChain allows them to create \"chains\" that automate the flow of information from ingestion to the final generated answer. \\nThe synergy between the two lies in how LangChain orchestrates the RAG pipeline. Using LangChain, a developer can quickly load diverse data formats like PDFs or website content, break them into manageable chunks, and store them as mathematical \"embeddings\" in a vector database for rapid searching. When a user asks a question, LangChain\\'s retrieval modules find the most relevant pieces of information and prompt the LLM to use that specific data to craft a factually grounded and contextually correct response. \\n'),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\rag_intro.txt'}, page_content='\\n    Retrieval-Augmented Generation (RAG) is an architectural framework designed to enhance the accuracy and reliability of Large Language Models (LLMs) by grounding them in external, authoritative knowledge bases. While standard models rely strictly on the data they were originally trained on—which can lead to outdated information or \"hallucinations\"—RAG enables a model to look up fresh data before generating a response. This process involves retrieving relevant document snippets from a source like a private database or the live web, and then passing that context into the model alongside the user’s original question.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Directory Loader ## Another way to load the data \n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "#Load all the text files from teh directory\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob = \"**/*.txt\" ,## Pattern to match files\n",
    "    loader_cls = TextLoader , #Loader class to use # if it is pdf loader use pdf \n",
    "    loader_kwargs = {'encoding' : 'utf-8'},\n",
    "    show_progress = False\n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca4dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Business_Analyst.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Business_Analyst.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Hema_Resume_Business_Analyst', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Hemasree Uppaluri Venkata Syamala \\n                                       uvs.hema@gmail.com | 602-813-8372 | LinkedIn| GitHub \\n \\nEDUCATION \\nMaster’s of Science in Information Technology                                                                                                         Aug’22 – May’24              \\nArizona State University | Arizona US \\nCoursework: Cloud Computing, Data Visualization, Statistical Machine Learning, Big Data Analytics, Project Management, Deep Learning, Natural \\nLanguage Processing \\n \\nTECHNICAL SKILLS \\nProgramming: Python, SQL, HTML, CSS \\nFrameworks/Tools: Pandas, NumPy, Scikit-learn, NLTK, Flask, Tableau, Power BI, SSIS, Git, VS Code \\nCloud Technologies and Databases: AWS (S3, EC2), Azure, Postgresql, Oracle, Microsoft SQL Server, Google Big Query, Google cloud functions \\nAI Practices: Natural Language Processing (NLP), Sentiment Analysis, Text Classification, Regression Analysis, Data Visualization, Predictive \\nModeling, Fast API’S , Pydantic Models, Kedros \\n \\nPROFESSIONAL EXPERIENCE \\nBusiness Analyst II , Rogue Lumen                                                                                                       March  2025 – Present \\n●\\u200b\\nLed competitive data synchronization by automating data extraction and pipeline processes (GCP BigQuery to Neo4j), which boosted \\n●\\u200b\\nreporting efficiency by 25% and ensured timely strategic analysis for Joor and Ziosk clients. \\n●\\u200b\\nAutomated key business communications using an AI-based solution that translated natural language into formatted content, reducing \\n●\\u200b\\nmanual processing time by 40% for critical client outreach. \\n●\\u200b\\nDrove compliance and operational efficiency by designing secure, scalable workflows for multi-source data ingestion, enhancing \\n●\\u200b\\nreporting speed and maintaining data governance standards across all client platforms.. \\n●\\u200b\\nImplemented scheduled Neo4j data extraction using Python and LLaMA tokenization for Ziosk, improving data freshness and update \\nefficiency by 30%. \\n●\\u200b\\nRefactored high-complexity SQL queries and ETL workflows, accelerating data retrieval speeds by 40% and reducing incident resolution \\ntime by 25% through optimized system logic. \\nBusiness Analyst – AI Development, Arizona State University                                                        May 2024 – Feb 2025 \\n●\\u200b\\nCollected and integrated data from 15+ nuclear power data centers across U.S. states, creating a centralized repository to support \\nregulatory reporting and performance benchmarking. \\n●\\u200b\\nDeveloped a scalable data pipeline to automate extraction from Excel, transformation in Python, and storage in MySQL and AWS \\nS3,reducing manual processing time by 70%. \\n●\\u200b\\nConducted market research and enhanced customer feedback analysis (via a sentiment pipeline), improving predictive planning accuracy by \\n●\\u200b\\n35% for state-wise cost and efficiency trends. \\n●\\u200b\\n Developed scalable processes to integrate critical data from diverse sources (Excel, MySQL, AWS S3), successfully reducing manual data \\n                preparation time by 70% for strategic analysis. \\nData Analyst , Tata Consultancy Pvt LTD | Hyderabad, India                                                          June 2019 – July 2022              \\n●\\u200b\\nDesigned a suite of 100+ interactive Tableau dashboards, automating real-time service metric tracking and eliminating 50% of manual \\nreporting overhead. \\n●\\u200b\\nRefactored high-complexity SQL queries across 15+ client projects, accelerating data retrieval speeds by 40% and optimizing database \\nperformance. \\n●\\u200b\\nSpearheaded cross-functional data audits to identify billing discrepancies and streamline ITSM workflows, yielding a 20% revenue uplift. \\n●\\u200b\\nEngineered data enhancement frameworks that reduced incident resolution time by 25%, improving system reliability and stakeholder trust. \\n●\\u200b\\nStandardized reporting protocols for TPG Telecom, transforming fragmented datasets into actionable executive insights through scalable \\nvisualization logic. \\nPROJECTS \\nStock Price Analysis with Machine Learning                                                                                                                              Dec’24 \\n●\\u200b\\nDeveloped a sentiment analysis pipeline using NLTK and Hugging Face Transformers to process financial news and social feeds. By \\nquantifying market sentiment,improved the correlation between news signals and short-term price movements, increasing prediction \\naccuracy over traditional baseline models.. \\nHousing Market Analysis Using Neo4j Graph Database                                                                                                                                     Nov’24    \\n●\\u200b\\nDesigned a Neo4j graph database model to analyze SF housing process data, capturing relationships between properties, neighborhoods, \\nand pricing trends. Leveraged graph queries to uncover insights and visualize housing market patterns for better decision-making.'), Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Data Analyst.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Data Analyst.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Hema_Resume_Data Analyst', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Hemasree Uppaluri Venkata Syamala \\n                                       uvs.hema@gmail.com | 602-813-8372 | LinkedIn| GitHub \\n \\nEDUCATION \\nMaster’s of Science in Information Technology                                                                                                         Aug’22 – May’24              \\nArizona State University | Arizona US \\nCoursework: Cloud Computing, Data Visualization, Statistical Machine Learning, Big Data Analytics, Project Management, Deep Learning, Natural \\nLanguage Processing \\n \\nTECHNICAL SKILLS \\nProgramming: Python, SQL, HTML, CSS \\nFrameworks/Tools: Pandas, NumPy, Scikit-learn, NLTK, Flask, Tableau, Power BI, SSIS, Git, VS Code \\nCloud Technologies and Databases: AWS (S3, EC2), Azure, Postgresql, Oracle, Microsoft SQL Server, Google Big Query, Google cloud functions \\nAI Practices: Natural Language Processing (NLP), Sentiment Analysis, Text Classification, Regression Analysis, Data Visualization, Predictive \\nModeling, Fast API’S , Pydantic Models, Kedros \\n \\nPROFESSIONAL EXPERIENCE \\nData Analyst , Rogue Lumen                                                                                                                  March  2025 – Present \\n●\\u200b\\nBuilt an end-to-end sentiment analysis pipeline in Python using NLP models to process multi-source API data, improving emotion \\nclassification accuracy by 30%. \\n●\\u200b\\nEstablished automated ingestion and ETL workflows across external APIs, BigQuery, PostgreSQL, and Neo4j, reducing manual processing \\nby 60% and accelerating reporting by 25%. \\n●\\u200b\\nDeveloped real-time dashboards to track sentiment trends and operational metrics, increasing user engagement by 25% and supporting \\nfaster, data-driven decisions. \\n●\\u200b\\nImplemented scheduled Neo4j data extraction using Python and LLaMA tokenization for Ziosk, improving data freshness and update \\nefficiency by 30%. \\n●\\u200b\\nRefactored high-complexity SQL queries and ETL workflows, accelerating data retrieval speeds by 40% and reducing incident resolution \\ntime by 25% through optimized system logic. \\nData Analyst Intern– AI Development, Arizona State University                                                          May 2024 – Feb 2025 \\n●\\u200b\\nCollected and integrated data from 15+ nuclear power data centers across U.S. states, creating a centralized repository to support \\nregulatory reporting and performance benchmarking. \\n●\\u200b\\nDeveloped a scalable data pipeline to automate extraction from Excel, transformation in Python, and storage in MySQL and AWS \\nS3,reducing manual processing time by 70%. \\n●\\u200b\\n Built AI-based email automation that converted natural language into markdown emails, reducing manual communication time by 40%, and \\nscraped manufacturer data using Scrapy, increasing data acquisition volume by 42% while maintaining compliance for Ara.  \\n●\\u200b\\n Streamlined secure, scalable workflows for multi-source data ingestion and automation, improving reporting speed, system efficiency, and \\ncompliance across client platforms \\nData Analyst , Tata Consultancy Pvt LTD | Hyderabad, India                                                          June 2019 – July 2022              \\n●\\u200b\\nDesigned a suite of 100+ interactive Tableau dashboards, automating real-time service metric tracking and eliminating 50% of manual \\nreporting overhead. \\n●\\u200b\\nRefactored high-complexity SQL queries across 15+ client projects, accelerating data retrieval speeds by 40% and optimizing database \\nperformance. \\n●\\u200b\\nSpearheaded cross-functional data audits to identify billing discrepancies and streamline ITSM workflows, yielding a 20% revenue uplift. \\n●\\u200b\\nEngineered data enhancement frameworks that reduced incident resolution time by 25%, improving system reliability and stakeholder trust. \\n●\\u200b\\nStandardized reporting protocols for TPG Telecom, transforming fragmented datasets into actionable executive insights through scalable \\nvisualization logic. \\nPROJECTS \\nStock Price Analysis with Machine Learning                                                                                                                              Dec’24 \\n●\\u200b\\nDeveloped a sentiment analysis pipeline using NLTK and Hugging Face Transformers to process financial news and social feeds. By \\nquantifying market sentiment,improved the correlation between news signals and short-term price movements, increasing prediction \\naccuracy over traditional baseline models.. \\nHousing Market Analysis Using Neo4j Graph Database                                                                                                                                     Nov’24    \\n●\\u200b\\nDesigned a Neo4j graph database model to analyze SF housing process data, capturing relationships between properties, neighborhoods, \\nand pricing trends. Leveraged graph queries to uncover insights and visualize housing market patterns for better decision-making.'), Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Data Engineer.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Data Engineer.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'Hema_Resume_Data Engineer', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Hemasree Uppaluri Venkata Syamala \\n                                       uvs.hema@gmail.com | 602-813-8372 | LinkedIn| GitHub \\n \\nEDUCATION \\nMaster’s of Science in Information Technology                                                                                                         Aug’22 – May’24              \\nArizona State University | Arizona US \\nCoursework: Cloud Computing, Data Visualization, Statistical Machine Learning, Big Data Analytics, Project Management, Deep Learning, Natural \\nLanguage Processing \\n \\nTECHNICAL SKILLS \\nProgramming: Python, SQL, HTML, CSS \\nFrameworks/Tools: Pandas, NumPy, Scikit-learn, NLTK, Flask, Tableau, Power BI, SSIS, Git, VS Code \\nCloud Technologies and Databases: AWS (S3, EC2), Azure, Postgresql, Oracle, Microsoft SQL Server, Google Big Query, Google cloud functions \\nAI Practices: Natural Language Processing (NLP), Sentiment Analysis, Text Classification, Regression Analysis, Data Visualization, Predictive \\nModeling, Fast API’S , Pydantic Models, Kedros \\n \\nPROFESSIONAL EXPERIENCE \\nData Engineer , Rogue Lumen                                                                                                               March  2025 – Present \\n●\\u200b\\nBuilt an end-to-end sentiment analysis pipeline in Python using NLP models to process multi-source API data, improving emotion \\nclassification accuracy by 30%. \\n●\\u200b\\nEstablished automated ingestion and ETL workflows across external APIs, BigQuery, PostgreSQL, and Neo4j, reducing manual processing \\nby 60% and accelerating reporting by 25%. \\n●\\u200b\\nDeveloped real-time dashboards to track sentiment trends and operational metrics, increasing user engagement by 25% and supporting \\nfaster, data-driven decisions. \\n●\\u200b\\nImplemented scheduled Neo4j data extraction using Python and LLaMA tokenization for Ziosk, improving data freshness and update \\nefficiency by 30%. \\n●\\u200b\\nRefactored high-complexity SQL queries and ETL workflows, accelerating data retrieval speeds by 40% and reducing incident resolution \\ntime by 25% through optimized system logic. \\nData Engineer Intern– AI Development, Arizona State University                                                        May 2024 – Feb 2025 \\n●\\u200b\\nCollected and integrated data from 15+ nuclear power data centers across U.S. states, creating a centralized repository to support \\nregulatory reporting and performance benchmarking. \\n●\\u200b\\nDeveloped a scalable data pipeline to automate extraction from Excel, transformation in Python, and storage in MySQL and AWS \\nS3,reducing manual processing time by 70%. \\n●\\u200b\\n Built AI-based email automation that converted natural language into markdown emails, reducing manual communication time by 40%, and \\nscraped manufacturer data using Scrapy, increasing data acquisition volume by 42% while maintaining compliance for Ara.  \\n●\\u200b\\n Streamlined secure, scalable workflows for multi-source data ingestion and automation, improving reporting speed, system efficiency, and \\ncompliance across client platforms \\nData Analyst , Tata Consultancy Pvt LTD | Hyderabad, India                                                          June 2019 – July 2022              \\n●\\u200b\\nDesigned a suite of 100+ interactive Tableau dashboards, automating real-time service metric tracking and eliminating 50% of manual \\nreporting overhead. \\n●\\u200b\\nRefactored high-complexity SQL queries across 15+ client projects, accelerating data retrieval speeds by 40% and optimizing database \\nperformance. \\n●\\u200b\\nSpearheaded cross-functional data audits to identify billing discrepancies and streamline ITSM workflows, yielding a 20% revenue uplift. \\n●\\u200b\\nEngineered data enhancement frameworks that reduced incident resolution time by 25%, improving system reliability and stakeholder trust. \\n●\\u200b\\nStandardized reporting protocols for TPG Telecom, transforming fragmented datasets into actionable executive insights through scalable \\nvisualization logic. \\nPROJECTS \\nStock Price Analysis with Machine Learning                                                                                                                              Dec’24 \\n●\\u200b\\nDeveloped a sentiment analysis pipeline using NLTK and Hugging Face Transformers to process financial news and social feeds. By \\nquantifying market sentiment,improved the correlation between news signals and short-term price movements, increasing prediction \\naccuracy over traditional baseline models.. \\nHousing Market Analysis Using Neo4j Graph Database                                                                                                                                     Nov’24    \\n●\\u200b\\nDesigned a Neo4j graph database model to analyze SF housing process data, capturing relationships between properties, neighborhoods, \\nand pricing trends. Leveraged graph queries to uncover insights and visualize housing market patterns for better decision-making.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "#Load all the pdf files from the directory\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf_files\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "pdf_documents = dir_loader.load()\n",
    "print(pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3528f65",
   "metadata": {},
   "source": [
    "  Second step in the process \n",
    "  Chuncking the data\n",
    "\n",
    "firstly, we load the pdfs we have uploaded on our system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "593aa77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: Hema_Resume_Business_Analyst.pdf\n",
      "  ✓ Loaded 1 pages\n",
      "\n",
      "Processing: Hema_Resume_Data Analyst.pdf\n",
      "  ✓ Loaded 1 pages\n",
      "\n",
      "Processing: Hema_Resume_Data Engineer.pdf\n",
      "  ✓ Loaded 1 pages\n",
      "\n",
      "Total documents loaded: 3\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58bc4cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 3 documents into 46 chunks\n",
      "\n",
      " Example chunk\n",
      "Content : Hemasree  Uppaluri  Venkata  Syamala                                         uvs.hema@gmail.com |  602-813-8372  |  LinkedIn|  GitHub...\n",
      "Metadata : {'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Hema_Resume_Business_Analyst', 'source': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Business_Analyst.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Hema_Resume_Business_Analyst.pdf', 'file_type': 'pdf'}\n",
      "Total chunks created: 46\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\" Split documents into smaller chunks\n",
    "    Args:\n",
    "        documents: List of Document objects or raw strings.\n",
    "        chunk_size: Max characters per chunk.\n",
    "        chunk_overlap: Overlap between chunks.\n",
    "    Returns:\n",
    "        List of Document chunks\n",
    "    \"\"\"\n",
    "    # Ensure all inputs are Document objects\n",
    "    if isinstance(documents[0], str):\n",
    "        documents = [Document(page_content=doc, metadata={}) for doc in documents]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=['\\n\\n', '\\n', ' ', '']\n",
    "    )\n",
    "    \n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # show example of chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\n Example chunk\")\n",
    "        print(f\"Content : {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata : {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "\n",
    "chunks = split_documents(all_pdf_documents)\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a96aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Embedding and vectorStoreDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6830558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52909c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class EmbeddingManger:\n",
    "    \"\"\"\n",
    "    Handles document embedding generation using SentenceTransformer\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"): # text into vectors\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "\n",
    "        Args:\n",
    "\n",
    "            model_name = HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Load the SentenceTransformer model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"model loaded successfully .Embedding Dimension : {self .model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name} : {e}\")\n",
    "            raise\n",
    "    def generate_embeddings(self , texts : List[str]) -> np.ndarray:\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts \n",
    "\n",
    "        Args: \n",
    "            text : list of text strings to embed\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        print (f\"Generating embeddings for {len (texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar = True)\n",
    "        print(f\"Generated embeddings with shape : {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094fa448",
   "metadata": {},
   "source": [
    "Intialize the embedding manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22b92b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "model loaded successfully .Embedding Dimension : 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManger at 0x186e3d73c40>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_manager = EmbeddingManger()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c555bf1",
   "metadata": {},
   "source": [
    "Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc461e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector store intialized . collection : pdf_documents\n",
      " Existing documents in collection : 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Vectorstore at 0x186e3d85eb0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Vectorstore:\n",
    "    \"\"\"Manages document embeddings in a chromadb vector store\"\"\"\n",
    "    def __init__(self , collection_name :str = 'pdf_documents' , persist_directory : str = \"../data/vector_store\"):\n",
    "\n",
    "        \"\"\" Intialize the vector store\n",
    "        Args:\n",
    "            collection_name : Name of the chromadb collection\n",
    "            persist_directory : Directory to persist the vectore store\n",
    "            \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Intiliaze chromadb client and collection\"\"\"\n",
    "        try:\n",
    "            #create persistent chromadb client\n",
    "            os.makedirs(self.persist_directory , exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path = self.persist_directory)\n",
    "\n",
    "            # get or create collection\n",
    "            #where we are going to store all teh vectors\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata= {\"description\" : \"PDF Document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"vector store intialized . collection : {self.collection_name}\")\n",
    "            print(f\" Existing documents in collection : {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error Initializing vectore store : {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents : List[Any], embeddings : np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vectore store\n",
    "            Args:\n",
    "            documents : List of Langchain documents\n",
    "            embeddings : Corresponding embeddings for the documents\n",
    "            \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        print(f\"Adding {len(documents)} documents to vector store \")\n",
    "\n",
    "        #prepare data for chromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i ,(doc , embedding) in enumerate(zip(documents , embeddings)):\n",
    "            #Generate unique ID \n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\" # id for specific variable\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length']= len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            #document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            #embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "            # Add to collection\n",
    "\n",
    "            try:\n",
    "                self.collection.add(\n",
    "                    ids = ids,\n",
    "                    embeddings= embeddings_list,\n",
    "                    metadatas=metadatas,\n",
    "                    documents= documents_text\n",
    "                )\n",
    "                print(f\"successfully added {len(documents)} documents to vector store\")\n",
    "                print(f\"Total documents in collection : {self.collection.count()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding documents to vector store ; {e}\")\n",
    "\n",
    "Vectorstore = Vectorstore()\n",
    "Vectorstore\n",
    "\n",
    "#to add documents call add_document def\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b777320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 46 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape : (46, 384)\n",
      "Adding 46 documents to vector store \n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 1\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 2\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 3\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 4\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 5\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 6\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 7\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 8\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 9\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 10\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 11\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 12\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 13\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 14\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 15\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 16\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 17\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 18\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 19\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 20\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 21\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 22\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 23\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 24\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 25\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 26\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 27\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 28\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 29\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 30\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 31\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 32\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 33\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 34\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 35\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 36\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 37\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 38\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 39\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 40\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 41\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 42\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 43\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 44\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 45\n",
      "successfully added 46 documents to vector store\n",
      "Total documents in collection : 46\n"
     ]
    }
   ],
   "source": [
    "### convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "texts\n",
    "\n",
    "##generate the embeddings\n",
    "\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store in the vector database\n",
    "Vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6a031",
   "metadata": {},
   "source": [
    "Retriver Pipeline From Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a61d0d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x186c5662340>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "\n",
    "    def __init__(self ,vector_store: Vectorstore ,embedding_managermbedding_manager :EmbeddingManger):\n",
    "        \"\"\"Intialize the retriever\n",
    "        Args\n",
    "            vectore_store = vector store containing document embeddings\n",
    "            embeddings_manager :Manager for generating query embeddings\n",
    "            \"\"\"\n",
    "        self.vector_store = Vectorstore\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "        ##retrive the data from specific query\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "\n",
    "        Args:\n",
    "            query : The search query\n",
    "            top_k :Number of top results to return\n",
    "            score_threshold :Minimum similarity score threshold\n",
    "        Returns: \n",
    "            List of dictionaries containg retrived documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query : '{query}'\")\n",
    "        print (f\"Top K : {top_k} , Score threshold : {score_threshold}\")\n",
    "\n",
    "        #generate query embedding\n",
    "        query_embeddings = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings = [query_embeddings.tolist()],\n",
    "                n_results = top_k\n",
    "            )\n",
    "\n",
    "            #process results\n",
    "            retrived_docs= []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results ['distances'][0]\n",
    "                ids = results ['ids'][0]\n",
    "\n",
    "                for i , (doc_id , document , metadata , distance) in enumerate(zip(ids, documents , metadatas , distances)):\n",
    "                    #convert the distance to similarity score(Chromadb uses cosine distance)\n",
    "                    similarity_score = 1- distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrived_docs .append(\n",
    "                            {\n",
    "                                'id' : doc_id,\n",
    "                                'content' :document,\n",
    "                                'metadata' : metadata,\n",
    "                                'similarity_score' : similarity_score,\n",
    "                                'distance' : distance,\n",
    "                                'rank' :i+1})\n",
    "                print(f\"Retrieved {len(retrived_docs)} documents (after filetring)\")\n",
    "                   \n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            return retrived_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrievak : {e}\")\n",
    "            return[]\n",
    "rag_retriver = RAGRetriever (Vectorstore , EmbeddingManger)\n",
    "rag_retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8efbba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query : 'can you give me technical skills'\n",
      "Top K : 5 , Score threshold : 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape : (1, 384)\n",
      "Retrieved 3 documents (after filetring)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_5ff19717_2',\n",
       "  'content': 'TECHNICAL  SKILLS  Programming :  Python,  SQL,  HTML,  CSS  Frameworks/Tools:  Pandas,  NumPy,  Scikit-learn,  NLTK,  Flask,  Tableau,  Power  BI,  SSIS,  Git,  VS  Code  Cloud  Technologies  and  Databases:  AWS  (S3,  EC2),  Azure,  Postgresql,  Oracle,  Microsoft  SQL  Server,  Google  Big  Query,  Google  cloud  functions  AI  Practices:  Natural  Language  Processing  (NLP),  Sentiment  Analysis,  Text  Classification,  Regression  Analysis,  Data  Visualization,  Predictive',\n",
       "  'metadata': {'creationdate': '',\n",
       "   'content_length': 485,\n",
       "   'page_label': '1',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Business_Analyst.pdf',\n",
       "   'producer': 'Skia/PDF m145 Google Docs Renderer',\n",
       "   'doc_index': 2,\n",
       "   'source_file': 'Hema_Resume_Business_Analyst.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 1,\n",
       "   'title': 'Hema_Resume_Business_Analyst',\n",
       "   'creator': 'PyPDF',\n",
       "   'page': 0},\n",
       "  'similarity_score': 0.055256783962249756,\n",
       "  'distance': 0.9447432160377502,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_518c5dd1_18',\n",
       "  'content': 'TECHNICAL  SKILLS  Programming :  Python,  SQL,  HTML,  CSS  Frameworks/Tools:  Pandas,  NumPy,  Scikit-learn,  NLTK,  Flask,  Tableau,  Power  BI,  SSIS,  Git,  VS  Code  Cloud  Technologies  and  Databases:  AWS  (S3,  EC2),  Azure,  Postgresql,  Oracle,  Microsoft  SQL  Server,  Google  Big  Query,  Google  cloud  functions  AI  Practices:  Natural  Language  Processing  (NLP),  Sentiment  Analysis,  Text  Classification,  Regression  Analysis,  Data  Visualization,  Predictive',\n",
       "  'metadata': {'creator': 'PyPDF',\n",
       "   'page': 0,\n",
       "   'doc_index': 18,\n",
       "   'source_file': 'Hema_Resume_Data Analyst.pdf',\n",
       "   'title': 'Hema_Resume_Data Analyst',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Data Analyst.pdf',\n",
       "   'content_length': 485,\n",
       "   'page_label': '1',\n",
       "   'producer': 'Skia/PDF m145 Google Docs Renderer',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 1,\n",
       "   'creationdate': ''},\n",
       "  'similarity_score': 0.055256783962249756,\n",
       "  'distance': 0.9447432160377502,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_ddae6068_33',\n",
       "  'content': 'TECHNICAL  SKILLS  Programming :  Python,  SQL,  HTML,  CSS  Frameworks/Tools:  Pandas,  NumPy,  Scikit-learn,  NLTK,  Flask,  Tableau,  Power  BI,  SSIS,  Git,  VS  Code  Cloud  Technologies  and  Databases:  AWS  (S3,  EC2),  Azure,  Postgresql,  Oracle,  Microsoft  SQL  Server,  Google  Big  Query,  Google  cloud  functions  AI  Practices:  Natural  Language  Processing  (NLP),  Sentiment  Analysis,  Text  Classification,  Regression  Analysis,  Data  Visualization,  Predictive',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'doc_index': 33,\n",
       "   'source_file': 'Hema_Resume_Data Engineer.pdf',\n",
       "   'page': 0,\n",
       "   'title': 'Hema_Resume_Data Engineer',\n",
       "   'producer': 'Skia/PDF m145 Google Docs Renderer',\n",
       "   'page_label': '1',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\Hema_Resume_Data Engineer.pdf',\n",
       "   'total_pages': 1,\n",
       "   'content_length': 485,\n",
       "   'creator': 'PyPDF',\n",
       "   'creationdate': ''},\n",
       "  'similarity_score': 0.055256783962249756,\n",
       "  'distance': 0.9447432160377502,\n",
       "  'rank': 3}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver.retrieve(\"can you give me technical skills\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uvshe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
